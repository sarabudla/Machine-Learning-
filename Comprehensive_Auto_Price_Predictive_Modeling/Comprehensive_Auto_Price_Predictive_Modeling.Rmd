---
title: "Practicum2.Rmd"
author: "Nithya sarabudla"
date: "2024-03-17"
output:
  html_document: default
  pdf_document: default
---

## Problem 1

2.Explore the dataset as you see fit and that allows you to get a sense of the data and get comfortable with it.

```{r Explore dataset}
# Setting seed for reproducibility
set.seed(111)

# Reading the data from a CSV file
mushroom_data <- read.csv("/Users/nithyasarabudla/DA5030/mushrooms.csv", na.strings="?")

# Checking the basic structure of the data
str(mushroom_data)
```

```{r dimensions}
# Checking the dimensions of the data
cat("Dimensions of the dataset", dim(mushroom_data), "\n")
```

```{r unique values exploration}
# Checking all the unique values from veil.type
unique(mushroom_data$veil.type)
```

As there is just one distinct value, it is better to drop this column

```{r stalk.root missing values}
sum(is.na(mushroom_data$stalk.root))
```

As there are more than 20% missing values in this column, it is better to drop this column too.

```{r remove unnecessary columns}
# Since the veil.type has just 1 distinct value and there are several missing values in stalk.root, dropping both the columns.
cols_to_remove <- c("veil.type", "stalk.root")
mushroom_data <- mushroom_data[, !names(mushroom_data) %in% cols_to_remove]
# Converting the processed data to data frame by converting the columns to factors
mushroom_data <- as.data.frame(lapply(mushroom_data, as.factor))
```

3.Split the combined data set 70/30% so you retain 30% for validation using random sampling without replacement. Use a fixed seed so you produce the same results each time you run the code. Going forward you will use the 70% data set for training and the 30% data set for validation and determine accuracy.

```{r split data}
# Using library caret for splitting
library(caret)
# Setting seed for reproductibility
set.seed(111)
# Splitting the data frame in 70% and 30% ratios without replacement
index <- createDataPartition(mushroom_data$class, p=0.7, list=FALSE)

df_train <- mushroom_data[index,]
df_valid <- mushroom_data[-index,]
```

```{r counts for class in training set}
# Checking value counts for class in training set
table(df_train$class)
```

```{r counts for class in validation set}
# Checking value counts for class in validation set
table(df_valid$class)
```

```{r dimensions of training and testing sets}
# Checking the dimensions of training and testing sets
cat("Dimensions of the training set:", dim(df_train), "\n")
cat("Dimensions of the testing set:", dim(df_valid), "\n")
```

4.Using the Naive Bayes Classification algorithm from the KlaR package, build a binary classifier that predicts the poisonous mushrooms. You need to transform continuous variables into categorical variables by binning, using equal size bins from min to max (ignore this if there is no numerical variables in the data).

```{r Naive Bayes Classification, warning=FALSE}
library(klaR)

# Building and fitting the Naive Bayes model usin training data
nb_model <- NaiveBayes(class ~ ., data = df_train)
# Validating using the validation data
nb_predictions <- predict(nb_model, newdata = df_valid)
```

5.Build a confusion matrix for the classifier from (4) and comment on it, e.g., explain what it means.

```{r Computing and Interpreting the Confusion Matrix Metrics}
# A function to compute various metrics using confusion matrix and print them
compute_metrics <- function(conf_matrix) {
  TP <- conf_matrix[2, 2]
  TN <- conf_matrix[1, 1]
  FP <- conf_matrix[1, 2]
  FN <- conf_matrix[2, 1]
  
  accuracy <- (TP + TN) / sum(conf_matrix)
  precision <- TP / (TP + FP)
  recall <- TP / (TP + FN)
  f1_score <- 2 * precision * recall / (precision + recall)
  
  cat("Accuracy:", accuracy, "\n")
  cat("Precision:", precision, "\n")
  cat("Recall:", recall, "\n")
  cat("F1 Score:", f1_score, "\n")
}
```

```{r Computing the confusion matrix and printing the metrics}
# Computing the confusion matrix and printing the metrics
confusion_matrix <- table(nb_predictions$class, df_valid$class)
print("NAIVE BAYES MODEL")
compute_metrics(confusion_matrix)
print(confusion_matrix)
```

- True Positives (TP): 1040 (correctly predicted poisonous mushrooms)
- True Negatives (TN): 1254 (correctly predicted non-poisonous mushrooms)
- False Positives (FP): 134 (incorrectly predicted as poisonous)
- False Negatives (FN): 8 (incorrectly predicted as non-poisonous)

The high number of true positives and true negatives indicates the model's ability to accurately classify mushrooms. The low number of false positives and false negatives suggests that the model's predictions are mostly correct, with only a few misclassifications.

6.Compare the results of Naive Bayes with the performance of RIPPER algorithm, which
algorithm works better on this dataset? how do you interpret the results?

```{r RIPPER Model}
# install.packages("C50")
library(C50)

# Building RIPPER model on the training data
ripper_model <- C5.0(df_train[, -which(names(df_train) == "class")], df_train$class)
# Validating using the validation data
ripper_predictions <- predict(ripper_model, newdata = df_valid)

# Computing the confusion matrix and printing the metrics
ripper_confusion_matrix <- table(ripper_predictions, df_valid$class)
print("RIPPER MODEL")
compute_metrics(ripper_confusion_matrix)
print(ripper_confusion_matrix)
```

RIPPER Model:

- True Positives (TP): 1174
- True Negatives (TN): 1262
- False Positives (FP): 0
- False Negatives (FN): 0

Comparison:

Both models have high numbers of true positives and true negatives, indicating their ability to correctly classify mushrooms.
The Naive Bayes model has a few false positives and false negatives, while the RIPPER model has none. This suggests that the RIPPER model is more conservative in its predictions.

Contrast:

The Naive Bayes model has a higher number of false positives and false negatives compared to the RIPPER model, which has none. This indicates that the Naive Bayes model may be slightly more prone to misclassification.

The RIPPER model's perfect classification in the validation set suggests that it may have overfit the training data, as it perfectly classifies even unseen data. This could be a concern if the model is not generalizing well to new data.

Overall, both models perform well, but the Naive Bayes model shows a few misclassifications, while the RIPPER model demonstrates perfect classification on the validation set, possibly indicating overfitting.



## Problem 2

2.Explore the dataset as you see fit and that allows you to get a sense of the data and get comfortable with it.

```{r Explore the dataset}
# Reading the data from a CSV file
bc_data <- read.csv("/Users/nithyasarabudla/DA5030/Wisonsin_breast_cancer_data.csv")
# Checking the basic structure of data
str(bc_data)
```

```{r missing values in data}
# Checking for the missing values in data.
as.matrix(colSums(is.na(bc_data)))
```

```{r Total missing values}
# Total missing values in data
sum(is.na(bc_data))
```

```{r Dimensions of the dataset}
cat("Dimensions of the dataset", dim(bc_data), "\n")
```

All the missing values are from this unknown `X` column which is completely empty. So, better to drop it. And the `id` column also serves no purpose in predicting the cancer state of an individual. So, dropping it.

```{r dropping column}
# Dropping both the "X" and "id" columns, then converting the label `diagnosis` to factor.
bc_data <- bc_data[, !(names(bc_data) == "X")]
bc_data <- bc_data[, !(names(bc_data) == "id")]
bc_data$diagnosis <- factor(bc_data$diagnosis)
# Printing the head
head(bc_data)
```

3.Split the combined data set 75/25% so you retain 25% for validation using random sampling without replacement. Use a fixed seed so you produce the same results each time you run the code. Going forward you will use the 75% data set for training and the 25% data set for validation and determine accuracy.

```{r split the data set}
# Setting seed for reproductibility
set.seed(111)
# Splitting the data frame in 75% and 25% ratios without replacement
index <- createDataPartition(bc_data$diagnosis, p=0.75, list=FALSE)

df_train <- bc_data[index,]
df_valid <- bc_data[-index,]
```

```{r counts Dimensions of the dataset }
# Checking value counts Dimensions of the dataset
table(df_train$diagnosis)
```

```{r counts of label in validation data}
# Checking value counts of label in validation data
table(df_valid$diagnosis)
```

```{r Checking dimensions}
# Checking dimensions
cat("Dimensions of the training set:", dim(df_train), "\n")
cat("Dimensions of the testing set:", dim(df_valid), "\n")
```

4.Create a full logistic regression model of the same features as in the original data (i.e., do not eliminate any features regardless of p-value) to predict the Diagnosis variable. Be sure to either use some encoding for categorical features or convert them to factor variables and ensure that the glm function does the dummy coding (ignore this if there is no categorical variable in the data).

```{r  logistic regression model on training data, warning=FALSE}
# Building and fitting the logistic regression model on training data
logit_model <- glm(diagnosis ~ ., data = df_train, family = binomial)
# Validating it using validation data
logit_predictions <- predict(logit_model, newdata = df_valid, type = "response")
```

5.Build a confusion matrix for the classifier from (4) and comment on it, e.g., explain what it means.

```{r confusion matrix }
# Classifying the probabilities into classes using 0.5 as threshold.
predicted_classes <- ifelse(as.numeric(logit_predictions) > 0.5, "M", "B")

# Computing the confusion matrix and printing the metrics
logit_confusion_matrix <- table(predicted_classes, df_valid$diagnosis)
print("LOGISTIC REGREESION MODEL")
compute_metrics(logit_confusion_matrix)
print(logit_confusion_matrix)
```
- The logistic regression model achieves high accuracy, precision, recall, and F1 score, indicating its effectiveness in classifying breast cancer cases as benign (B) or malignant (M).
- With a low number of false positives (2) and false negatives (5), the model demonstrates strong performance in correctly identifying both benign and malignant cases.
- The high precision indicates that when the model predicts a case as malignant, it is correct approximately 96.23% of the time, reducing the chances of unnecessary treatment for benign cases.
- The recall score of 91.07% indicates that the model effectively identifies the majority of actual malignant cases.

6.Create a Decision Tree model from rpart package, build a classifier that predicts the Diagnosis variable.

```{r Decision Tree model from rpart package}
# Importing the required library for decision tree
library(rpart)

# Building and fitting tree model on training data
tree_model <- rpart(diagnosis ~ ., data = df_train)
# Plotting the tree
plot(tree_model)
text(tree_model, cex = 0.7)
```

7.Build a confusion matrix for the classifier from (6) and comment on it, e.g., explain what it means.

```{r Decision tree model}
# Validating tree model on validation data
tree_predictions <- predict(tree_model, newdata = df_valid, type = "class")

# Computing the confusion matrix and printing the metrics
tree_confusion_matrix <- table(tree_predictions, df_valid$diagnosis)
print("DECISION TREE MODEL")
compute_metrics(tree_confusion_matrix)
print(tree_confusion_matrix)
```

- The decision tree model achieves good accuracy, precision, recall, and F1 score, indicating its effectiveness in classifying breast cancer cases as benign (B) or malignant (M).
- The model shows a slightly higher number of false positives (5) compared to the logistic regression model, suggesting that it may be less conservative in classifying cases as malignant.
- However, the decision tree model also has a slightly higher number of false negatives (6) compared to the logistic regression model, indicating that it may miss a few malignant cases.

8.Use a boosting ensemble (C5.0 decision tree with 10 boosting iterations) to predict the
Diagnosis variable.

```{r boosting ensemble model}
# Importing the required library for boosting model
library(C50)

set.seed(111)
# Train the boosting ensemble model with 10 iterations
boost_model <- C5.0(diagnosis ~ ., data = df_train, trials = 10)
```

9.Build a confusion matrix for the classifier from (8) and comment on it, e.g., explain what it means.

```{r Boosting model}
# Make predictions on the validation set
boost_predictions <- predict(boost_model, newdata = df_valid)

# Computing the confusion matrix and printing the metrics
boost_confusion_matrix <- table(boost_predictions, df_valid$diagnosis)
print("BOOSTING MODEL")
compute_metrics(boost_confusion_matrix)
print(boost_confusion_matrix)
```

- The model shows very few false positives (1) and false negatives (2), suggesting that it is excellent at correctly classifying both benign and malignant cases.
- The high precision indicates that when the model predicts a case as malignant, it is correct approximately 98.11% of the time, reducing the chances of unnecessary treatment for benign cases.
- The recall score of 96.30% indicates that the model effectively identifies the majority of actual malignant cases.

10. Build a function called predictOutcomeClass() that predicts the same Outcome variable and that combines the three predictive models from (4), (6), and (8) into a simple ensemble and uses majority vote to determine the final prediction using the individual predictions.

```{r predict outcome class based on all three models}
set.seed(111)
# A function to predict outcome class based on all three models.
predictOutcomeClass <- function(logistic_model, tree_model, boost_model, data) {
  # Predict using logistic regression model
  logistic_pred <- predict(logistic_model, newdata = data, type = "response")
  logistic_pred <- ifelse(logistic_pred > 0.5, "M", "B")
  
  # Predict using decision tree model
  tree_pred <- predict(tree_model, newdata = data, type = "class")
  
  # Predict using boosting ensemble model
  boost_pred <- predict(boost_model, newdata = data)
  
  # Combine predictions into a data frame
  predictions_df <- data.frame(Logistic = logistic_pred, Tree = tree_pred, Boost = boost_pred)
  
  # Use row-wise majority vote to determine final prediction
  final_prediction <- apply(predictions_df, 1, function(x) {
    tab <- table(x)
    as.character(names(tab)[which.max(tab)])
  })
  
  return(final_prediction)
}
```

11.Using the ensemble model from (10), predict the Diagnosis of the following individual (you can impute the missing values/columns using median): Radius_mean: 14.5 | Texture_mean: 17.0 | Perimeter_mean: 87.5 | Area_mean: 561.3 | Smoothness_mean: 0.098 | Compactness_mean: 0.105 | Concavity_mean: 0.085 | Concave_points_mean: 0.050 | Symmetry_mean: 0.180 | Fractal_dimension_mean: 0.065 | Radius_se: 0.351 | Texture_se: 1.015 | Perimeter_se: 2.457 | Area_se: 26.15 | Smoothness_se: 0.005 | Compactness_se: 0.022 | Concavity_se: 0.036 | Concave_points_se: 0.013 | Symmetry_se: 0.030 | Fractal_dimension_se: 0.005 | Radius_worst: 16.5 | Texture_worst: 25.3 | Perimeter_worst: 114.8 | Area_worst: 733.5 | Smoothness_worst: 0.155 | Compactness_worst: 0.220 | Concavity_worst: missing | Concave_points_worst: missing | Symmetry_worst: 0.360 | Fractal_dimension_worst: 0.110

```{r  predict the Diagnosis}
individual <- data.frame(
  radius_mean = 14.5,
  texture_mean = 17.0,
  perimeter_mean = 87.5,
  area_mean = 561.3,
  smoothness_mean = 0.098,
  compactness_mean = 0.105,
  concavity_mean = 0.085,
  concave.points_mean = 0.050,
  symmetry_mean = 0.180,
  fractal_dimension_mean = 0.065,
  radius_se = 0.351,
  texture_se = 1.015,
  perimeter_se = 2.457,
  area_se = 26.15,
  smoothness_se = 0.005,
  compactness_se = 0.022,
  concavity_se = 0.036,
  concave.points_se = 0.013,
  symmetry_se = 0.030,
  fractal_dimension_se = 0.005,
  radius_worst = 16.5,
  texture_worst = 25.3,
  perimeter_worst = 114.8,
  area_worst = 733.5,
  smoothness_worst = 0.155,
  compactness_worst = 0.220,
  concavity_worst = NA,  # missing value
  concave.points_worst = NA,  # missing value
  symmetry_worst = 0.360,
  fractal_dimension_worst = 0.110
)

# Impute missing values with median
individual$concavity_worst <- median(df_train$concavity_worst, na.rm = TRUE)
individual$concave.points_worst <- median(df_train$concave.points_worst, na.rm = TRUE)

# Predict the diagnosis using the ensemble model
prediction <- predictOutcomeClass(logit_model, tree_model, boost_model, individual)

# Print the prediction
cat("Prediction:", prediction, "\n")
```

The outcome is Benign class.

12. (3 bonus points) build a Random Forest ensemble with 100 trees and compare it's results over the test split with the boosting ensemble from (8).

```{r random forest model}
# Import required library for random forest model
library(randomForest)

# Train the Random Forest model on training data
rf_model <- randomForest(diagnosis ~ ., data = df_train, ntree = 100)

# Make predictions on the validation set
rf_predictions <- predict(rf_model, newdata = df_valid)

# Calculate accuracy for Random Forest
rf_accuracy <- mean(rf_predictions == df_valid$diagnosis)
cat("Random Forest Accuracy:", rf_accuracy, "\n")

# Make predictions using the boosting ensemble model
boost_predictions <- predict(boost_model, newdata = df_valid)

# Calculate accuracy for Boosting Ensemble
boost_accuracy <- mean(boost_predictions == df_valid$diagnosis)
cat("Boosting Ensemble Accuracy:", boost_accuracy, "\n")
```

The Random Forest model achieves an accuracy of 97.18%, while the Boosting Ensemble model achieves a slightly higher accuracy of 97.89%. This indicates that the Boosting Ensemble model performs slightly better in classifying breast cancer cases compared to the Random Forest model.

## Problem 3

```{r load the data}
# Importing the data from downloaded data, where NAs are represented by "?" character.
cars.df <- read.csv("/Users/nithyasarabudla/DA5030/automobile/imports-85.data", na.strings = "?")

# As the columns names are not provided in the data, those are manually set.
colnames(cars.df) <- c(
  "symboling", "normalized_losses", "make", "fuel_type", "aspiration", 
  "num_of_doors", "body_style", "drive_wheels", "engine_location", 
  "wheel_base", "length", "width", "height", "curb_weight", 
  "engine_type", "num_of_cylinders", "engine_size", "fuel_system", 
  "bore", "stroke", "compression_ratio", "horsepower", "peak_rpm", 
  "city_mpg", "highway_mpg", "price"
)
# Checking the basic structure of data
str(cars.df)
```

```{r Selecting Required Columns and Filtering the Data}
# Required categorical columns
categorical_columns <- c("num_of_doors", "num_of_cylinders", "engine_location")
# All numeric type columns
numeric_columns <- sapply(cars.df, is.numeric)
# All integer type columns
integer_columns <- sapply(cars.df, is.integer)
# merging integer type and numeric type into one.
num_int_columns <- names(cars.df)[numeric_columns | integer_columns]
# Finally, merging all the required columns
selected_columns <- c(categorical_columns, num_int_columns)

# Filtering the required columns.
cars.df <- cars.df[, selected_columns]
```

```{r Checking for Missing Values in the Data}
as.matrix(sapply(cars.df, FUN = function (x) sum(is.na(x))))
```

```{r Imputing Missing Values in Numeric/Integer Columns}
# For numeric/integer columns, impute the missing values with the column's median value.
for (col in num_int_columns) {
  cars.df[[col]] <- ifelse(is.na(cars.df[[col]]), median(cars.df[[col]], na.rm = TRUE), cars.df[[col]])
}
```

```{r Imputing Missing Values in Categorical Columns}
# For the categorical data, the missing value is present in "num_of_doors" column, it is better to impute them with most frequent value.
mode_num_of_doors <- names(which.max(table(cars.df$num_of_doors)))
cars.df$num_of_doors[is.na(cars.df$num_of_doors)] <- mode_num_of_doors
```

```{r Checking Total Missing Values After Imputation}
cat("Total missing values after imputation:", sum(is.na(cars.df)), "\n")
```

```{r Encoding Categorical Features with Numeric Values}
# Encode the written numbers with actual numeric numbers.
door_mapping <- c("two" = 2, "four" = 4)
cylinder_mapping <- c("two" = 2, "three" = 3, "four" = 4, "five" = 5, "six" = 6, "eight" = 8, "twelve" = 12)

cars.df$num_of_doors <- as.integer(door_mapping[cars.df$num_of_doors])
cars.df$num_of_cylinders <- as.integer(cylinder_mapping[cars.df$num_of_cylinders])
# Factorize the engine location column.
cars.df$engine_location <- as.integer(factor(cars.df$engine_location, levels = c("front", "rear")))
```

```{r Checking the Structure and Types of Data}
# Checking the structure and types of data
str(cars.df)
```

2. Are there outliers in any one of the features in the data set? How do you identify outliers? Remove them but create a second data set with outliers removed called cars.no.df. Keep the original data set cars.df.

```{r identifing and removing the outliers}
# Duplicate cars.df into cars.no.df
cars.no.df <- cars.df

# Function to detect outliers using IQR method, ignoring specified columns
detect_outliers_ignore <- function(x) {
  q1 <- quantile(x, 0.25)
  q3 <- quantile(x, 0.75)
  # Compute the inter quartile range
  iqr <- q3 - q1
  lower_bound <- q1 - 1.5 * iqr
  upper_bound <- q3 + 1.5 * iqr
  outliers <- x[x < lower_bound | x > upper_bound]
  return(list(count = length(outliers), outliers = outliers))
}

# Iterate over each column and remove outliers from cars.no.df
for (col in names(cars.no.df)) {
  # If the column is categorical, ignore it.
  if (!(col %in% categorical_columns)) {
    outliers_info <- detect_outliers_ignore(cars.no.df[[col]])
    cat("Outliers in", col, ":", outliers_info$count, "\n")
    # Remove the outlier rows from cars.no.df
    cars.no.df <- cars.no.df[!cars.no.df[[col]] %in% outliers_info$outliers, ]
  }
}
```

The Interquartile Range (IQR) is a robust measure of statistical dispersion that is often used to identify and remove outliers from a dataset. To calculate the IQR, the dataset is first divided into quartiles, with the first quartile (Q1) representing the value below which 25% of the data falls, and the third quartile (Q3) representing the value below which 75% of the data falls. The IQR is then calculated as the difference between Q3 and Q1. Outliers are typically defined as values that fall below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR, indicating that they are unusually high or low compared to the rest of the data. By identifying and removing outliers using the IQR, the dataset can be cleaned to improve the accuracy and reliability of statistical analyses and machine learning models.

```{r Checking Number of Unique Values in Each Column After Outlier Removal}
cat("Number of Unique values in each column: \n")
for (col in names(cars.no.df)) {
  cat(col, ":", length(unique(cars.no.df[, col])), "\n")
}
```
There is only 1 unique value in `engine_location` after removing the outliers. We can drop this column.
```{r Dropping the engine_location Column with Only One Unique Value}
# Dropping the engine location from cars.no.df
cars.no.df$engine_location <- NULL
```

Using pairs.panel , what are the distributions of each of the features in the data set with outliers removed (cars.no.df)? Are they reasonably normal so you can apply a statistical learner such as regression? Can you normalize features through a log, inverse, or square-root transform? State which features should be transformed and then transform as needed and build a new data set, cars.tx.

```{r Analyzing Feature Distributions Using pairs.panels, message=FALSE}
# Importing the required packages for pairs.panels
# install.packages("psych")
library(psych)
# Plot the cars.no.df using pairs.panels
pairs.panels(cars.no.df)
```

```{r Building a Linear Regression Model and Summarizing It}
# Build a linear regression model
model <- lm(price ~ ., data = cars.no.df)
summary(model)
```

- The overall model seems to be significant with a p-value less than 0.05, indicating that at least one of the predictors has a significant effect on the price.
- The Adjusted R-squared value of 0.7794 suggests that the model explains about 77.94% of the variance in the response variable, which is quite good.
- Looking at the coefficients, we see that 'horsepower' has a strong positive effect on price, as indicated by its high coefficient and low p-value. This suggests that as 'horsepower' increases, the price of the car tends to increase.
- 'num_of_doors', 'symboling', 'length', 'width', 'height', 'curb_weight', 'engine_size', 'bore', 'stroke', 'compression_ratio', 'peak_rpm', 'city_mpg', and 'highway_mpg' do not seem to have a significant effect on price, as their p-values are higher than 0.05.
- It's also worth noting that 'num_of_cylinders' has a marginally significant effect on price, with a p-value of 0.0944.

```{r Identifying and Transforming Skewed Columns}
# Importing required library for computing skewness
library(e1071)

# Identify skewed columns
skewed_columns <- names(Filter(function(x) abs(skewness(x, na.rm = TRUE)) > 0.5, cars.no.df))

# Apply log transformation to skewed columns
cars.tx <- cars.no.df
cars.tx[skewed_columns] <- lapply(cars.tx[skewed_columns], function(x) log(x + 1))
```

4.What are the correlations to the response variable (price) for cars.no.df? Are there collinearities? Build a full correlation matrix.

```{r Analyzing Correlations to the Response Variable (Price)}
# Calculate correlations
correlations <- cor(cars.no.df)

# Correlation with the response variable (price)
price_correlations <- correlations["price", ]
print(price_correlations)
```

These correlations reveal the relationships between various car features and their prices in the dataset. Features like engine size, curb weight, length, width, and height show strong positive correlations with price, indicating that larger and heavier cars tend to be more expensive. Similarly, horsepower exhibits a strong positive correlation, suggesting that cars with higher horsepower command higher prices. On the other hand, city and highway miles per gallon (MPG) exhibit strong negative correlations with price, indicating that cars with better fuel efficiency are often more affordable.

```{r Visualizing the Full Correlation Matrix}
library(corrplot)

corrplot(correlations, method="number", number.cex=0.4, tl.cex=0.8)
```

5. Split each of the three data sets, cars.no.df, cars.df, and cars.tx 70%/30% so you retain 30% for testing using random sampling without replacement. Call the datasets, cars.training and cars.testing, cars.no.training and cars.no.testing, and cars.tx.training and cars.tx.testing.

```{r Splitting Data into Training and Testing Sets}
# Set seed for reproductibility
set.seed(111)

# Split cars.df
index <- createDataPartition(cars.df$price, p = 0.7, list = FALSE)
cars.training <- cars.df[index, ]
cars.testing <- cars.df[-index, ]

# Split cars.no.df
index <- createDataPartition(cars.no.df$price, p = 0.7, list = FALSE)
cars.no.training <- cars.no.df[index, ]
cars.no.testing <- cars.no.df[-index, ]

# Split cars.tx
index <- createDataPartition(cars.tx$price, p = 0.7, list = FALSE)
cars.tx.training <- cars.tx[index, ]
cars.tx.testing <- cars.tx[-index, ]
```

6. Build three ideal multiple regression models for cars.training, cars.no.training, and cars.tx.training using backward elimination based on p-value for predicting price.

```{r Building Multiple Regression Models Using Backward Elimination, results='hide'}
# Building and Fitting multiple regression models on training datasets

# For cars.training
model_training <- lm(price ~ ., data = cars.training)
final_model_training <- step(model_training, direction = "backward")

# For cars.no.training
model_no_training <- lm(price ~ ., data = cars.no.training)
final_model_no_training <- step(model_no_training, direction = "backward")

# For cars.tx.training
model_tx_training <- lm(price ~ ., data = cars.tx.training)
final_model_tx_training <- step(model_tx_training, direction = "backward")
```

7. Build a Regression Tree model using rpart package for predicting price: one with cars.training, one with cars.no.training, and one with cars.tx.training.

```{r Building Regression Tree Models for Price Prediction}
library(rpart)

# Build a regression tree model for cars.training
tree_model_training <- rpart(price ~ ., data = cars.training, method = "anova")

# Build a regression tree model for cars.no.training
tree_model_no_training <- rpart(price ~ ., data = cars.no.training, method = "anova")

# Build a regression tree model for cars.tx.training
tree_model_tx_training <- rpart(price ~ ., data = cars.tx.training, method = "anova")
```

8. Provide an analysis of all the 6 models (using their respective testing data sets), including Adjusted R-Squared and RMSE. Which of these models is the best? Why?

```{r Analyzing All Models with Adjusted R-Squared and RMSE, results='hide'}
# Define a function to calculate RMSE
rmse <- function(predicted, actual) {
  sqrt(mean((predicted - actual)^2))
}

# Define a function to extract adjusted R2 score
adj_r_squared <- function(model, data) {
  1 - (1 - summary(model)$adj.r.squared) * ((nrow(data) - 1) / (nrow(data) - length(model$coefficients) - 1))
}

# Calculate RMSE for each model using their testing datasets
results <- data.frame(
  Model = c("Multiple Regression (cars.training)", "Multiple Regression (cars.no.training)", "Multiple Regression (cars.tx.training)",
            "Regression Tree (cars.training)", "Regression Tree (cars.no.training)", "Regression Tree (cars.tx.training)"),
  Adjusted_R_Squared = c(
    adj_r_squared(final_model_training, cars.testing),
    adj_r_squared(final_model_no_training, cars.no.testing),
    adj_r_squared(final_model_tx_training, cars.tx.testing),
    adj_r_squared(tree_model_training, cars.testing),
    adj_r_squared(tree_model_no_training, cars.no.testing),
    adj_r_squared(tree_model_tx_training, cars.tx.testing)
  ),
  RMSE = c(
    rmse(predict(final_model_training, newdata = cars.testing), cars.testing$price),
    rmse(predict(final_model_no_training, newdata = cars.no.testing), cars.no.testing$price),
    rmse(predict(final_model_tx_training, newdata = cars.tx.testing), cars.tx.testing$price),
    rmse(predict(tree_model_training, newdata = cars.testing), cars.testing$price),
    rmse(predict(tree_model_no_training, newdata = cars.no.testing), cars.no.testing$price),
    rmse(predict(tree_model_tx_training, newdata = cars.tx.testing), cars.tx.testing$price)
  )
)

# Print the results
print(results)

```
Based on the obtained results, the best model for predicting price appears to be the Multiple Regression model trained on the cars.tx.training dataset. This model has the highest Adjusted R-Squared value of 0.7902639, indicating a good fit to the data. Additionally, it has a very low RMSE (Root Mean Squared Error) of 0.1690655, suggesting that it has accurate predictions compared to the actual prices in the testing dataset.

9. Using each of the regression models, how one unit change in highway-mpg translates into the price prediction? how about city-mpg? (do not apply backward feature elimination for only for this part).

```{r Interpreting Coefficients for highway_mpg and city_mpg Features}
# For Multiple Regression models
coef_training <- coef(model_training)
coef_no_training <- coef(model_no_training)
coef_tx_training <- coef(model_tx_training)

# Extract coefficients for highway-mpg and city-mpg
coef_highway_mpg_training <- coef_training["highway_mpg"]
coef_highway_mpg_no_training <- coef_no_training["highway_mpg"]
coef_highway_mpg_tx_training <- coef_tx_training["highway_mpg"]

coef_city_mpg_training <- coef_training["city_mpg"]
coef_city_mpg_no_training <- coef_no_training["city_mpg"]
coef_city_mpg_tx_training <- coef_tx_training["city_mpg"]

# Interpretation
cat("Multiple Regression (cars.training):\n")
cat("One unit change in highway-mpg leads to a change of", coef_highway_mpg_training, "in price.\n")
cat("One unit change in city-mpg leads to a change of", coef_city_mpg_training, "in price.\n\n")

cat("Multiple Regression (cars.no.training):\n")
cat("One unit change in highway-mpg leads to a change of", coef_highway_mpg_no_training, "in price.\n")
cat("One unit change in city-mpg leads to a change of", coef_city_mpg_no_training, "in price.\n\n")

cat("Multiple Regression (cars.tx.training):\n")
cat("One unit change in highway-mpg leads to a change of", coef_highway_mpg_tx_training, "in price.\n")
cat("One unit change in city-mpg leads to a change of", coef_city_mpg_tx_training, "in price.\n\n")

```

10. For each of the predictions, calculate the 95% prediction interval for the price. (Exclude Regression Trees)

```{r Calculating 95% Prediction Interval for Price}
calculate_prediction_interval <- function(model, data) {
  # Predicted values
  predictions <- predict(model, newdata = data, interval = "prediction" ,level = 0.95)
  # Prediction interval
  prediction_interval <- data.frame(
    Lower = predictions[, "lwr"],
    Upper = predictions[, "upr"]
  )
  return(prediction_interval)
}


prediction_interval_training <- calculate_prediction_interval(final_model_training, cars.training)
prediction_interval_no_training <- calculate_prediction_interval(final_model_no_training, cars.no.training)
prediction_interval_tx_training <- calculate_prediction_interval(final_model_tx_training, cars.tx.training)

# Print the prediction intervals
cat("95% Prediction Interval for Multiple Regression (cars.training):\n")
print(head(prediction_interval_training))

cat("\n95% Prediction Interval for Multiple Regression (cars.no.training):\n")
print(head(prediction_interval_no_training))

cat("\n95% Prediction Interval for Multiple Regression (cars.tx.training):\n")
print(head(prediction_interval_tx_training))
```
Each data frame explains the 95% prediction intervals for price for each of the three multiple regression models.