---
title: "performing OCR with SVMs"
author: "Nithya Sarabudla"
date: "03-24-2024"
output:
  pdf_document: default
  html_notebook: default
---

## Step 2 - exploring and preparng the data 
```{r}
# load the data 
letters <- read.csv("/Users/nithyasarabudla/Downloads/letterdata.csv", stringsAsFactors = TRUE)
# Display the structure of the letters data
str(letters)
```

The dataset comprises 20,000 examples of 26 capital English letters, represented through 16 statistical features derived from images.These attributes include measures such as the horizontal and vertical dimensions of the glyph, the proportion of black versus white pixels, and the average position of pixels both horizontally and vertically. 

```{r}
# Split the letter data into training and testing sets
letters_train <- letters[1:16000, ]
letters_test  <- letters[16001:20000, ]
```

The dataset is split into training and testing subsets. The first 16,000 records are assigned to letters_train for training the model, and the next 4,000 records are allocated to letters_test for evaluating the model's performance.

## Step 3 - training a model on the data 
```{r}
# Load the kernlab library for support vector machine modeling
library(kernlab)
# Train a support vector machine classifier using the vanilladot kernel
letter_classifier <- ksvm(letter ~ ., data = letters_train,
                            kernel = "vanilladot")
letter_classifier
```
The kernlab library is loaded to provide functions for support vector machine (SVM) modeling. An SVM classifier is trained on the training dataset using a linear kernel (vanilladot). The model aims to classify letters based on the given attributes. After training, the model's summary is displayed, which includes information about the type of SVM, the kernel used, the number of support vectors, and the training error.

## step 4 - evaluating model performance 
```{r}
# Make predictions on the testing data using the trained classifier
letter_predictions <- predict(letter_classifier, letters_test)
head(letter_predictions)
```
The previously trained SVM classifier to make predictions on the testing set. The predict() function applies the model to new data and returns the predicted letter classifications. The head() function then displays the first few predictions to provide a quick look at the results.we can see that the first six predicted letters were U, N, V, X, N, and H
```{r}
# Create a contingency table to compare predicted and actual letters
table(letter_predictions, letters_test$letter)
```
A contingency table is created to compare the predicted letters against the actual letters from the testing set. This table helps in visually assessing which letters are correctly identified and where misclassifications occur.

```{r}
# Check the agreement between predicted and actual lettersa
agreement <- letter_predictions == letters_test$letter
table(agreement)
```
calculates whether each prediction matches the actual letter and summarizes the results in a table, showing the number of correct and incorrect predictions.
Using the table() function, we see that the classifier correctly identified the letter in 3,357 out of the 4,000 test records
```{r}
# Calculate the proportion of agreement
prop.table(table(agreement))
```
The proportion of correct predictions (agreement between the predicted and actual letters) is calculated to quantify the model's accuracy on the testing set.The accuracy is about 80 percent.

step 5 - improving model performance 

# Changing the SVM kernel function
```{r}
set.seed(12345)
letter_classifier_rbf <- ksvm(letter ~ ., data = letters_train,
                                kernel = "rbfdot")
```
To potentially improve model performance, this chunk trains a new SVM classifier using the Gaussian RBF (Radial Basis Function) kernel instead of the linear kernel. Setting a seed ensures that results are reproducible.

```{r}
# Make predictions using the SVM classifier with RBF kernel
letter_predictions_rbf <- predict(letter_classifier_rbf,
                                    letters_test)
```
Predictions are made on the testing set using the RBF kernel-based SVM model to evaluate its performance.

```{r}
# Check the agreement between predicted and actual letters using the RBF kernel
agreement_rbf <- letter_predictions_rbf == letters_test$letter
table(agreement_rbf)
```
Assesses the accuracy of the RBF kernel-based model by comparing the predicted letters against the actual ones and summarizing the results in a table.

```{r}
# Calculate the proportion of agreement using the RBF kernel
prop.table(table(agreement_rbf))
```
Calculates the proportion of correct predictions made by the RBF kernel-based SVM model, providing a clear measure of its accuracy. Able to increase the accuracy of our character recognition model from 84 percent to 93 percent.

# Identifying the best SVM cost parameter
```{r}
# Identifying the best SVM cost parameter
cost_values <- c(1, seq(from = 5, to = 40, by = 5))
accuracy_values <- sapply(cost_values, function(x) {
    set.seed(12345)
    m <- ksvm(letter ~ ., data = letters_train,
              kernel = "rbfdot", C = x)
    pred <- predict(m, letters_test)
    agree <- ifelse(pred == letters_test$letter, 1, 0)
    accuracy <- sum(agree) / nrow(letters_test)
    return (accuracy)
  })
plot(cost_values, accuracy_values, type = "b")
```

In the quest to optimize the performance of an SVM model for optical character recognition (OCR), the exploration of the cost parameter, C, plays a crucial role. The cost parameter influences the trade-off between model complexity and the degree to which deviations from a perfect classification are tolerated. By experimenting with a range of cost values, from 1 to 40, the model's sensitivity to this parameter is assessed. This process is facilitated through a systematic approach using the sapply() function to evaluate model accuracy across different C values. The outcome reveals that increasing C from the default value of 1 to 10 or beyond significantly enhances accuracy, reaching up to 97%.
