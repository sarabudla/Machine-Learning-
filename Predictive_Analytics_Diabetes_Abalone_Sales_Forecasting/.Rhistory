y = "Density") +
theme_minimal()
# Perform the Shapiro-Wilk test for normality on the Glucose column
shapiro_test_result <- shapiro.test(diabetes$Glucose)
# Print the result of the Shapiro-Wilk test
print(shapiro_test_result)
# Define the variables
selected_vars <- c("Pregnancies", "Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI", "DiabetesPedigreeFunction", "Age")
# Apply a function to each variable in the 'diabetes' dataset
outliers <- lapply(diabetes[selected_vars], function(var) {
# Calculate mean and standard deviation of the variable, ignoring NA values
var_mean <- mean(var, na.rm=TRUE)
var_sd <- sd(var, na.rm=TRUE)
# Calculate z-scores for each observation
z_scores <- abs((var - var_mean) / var_sd)
# Identify outliers based on z-score threshold (2.5)
outliers_index <- which(z_scores > 2.5)
# Create a data frame with indices and corresponding z-scores for outliers
return(data.frame(Indices = outliers_index, Z_Scores = z_scores[outliers_index]))
})
# Define the function for z-score standardization
standardize <- function(column) {
# Subtract the mean and divide by the standard deviation
(column - mean(column, na.rm = TRUE)) / sd(column, na.rm = TRUE)
}
# Create a copy of the original dataset
standardized_data <- diabetes
# Apply z-score standardization to all numeric columns using the defined function
standardized_data[selected_vars] <- lapply(diabetes[selected_vars], standardize)
# Extract indices of all outliers across all variables
all_outlier_indices <- unique(unlist(lapply(outliers, function(df) df$Indices)))
# Remove rows containing outliers
cleaned_data <- diabetes[-all_outlier_indices, ]
# Apply z-score standardization again to cleaned dataset
cleaned_data[selected_vars] <- lapply(cleaned_data[selected_vars], standardize)
# Print summary statistics of the original dataset
summary(diabetes)
# Set seed for reproducibility
set.seed(123)
# Create stratified sampling indices
split <- createDataPartition(cleaned_data$Outcome, p = 0.8, list = FALSE)
# Split data into training and validation sets based on the sampling indices
training_data <- cleaned_data[split, ]
validation_data <- cleaned_data[-split, ]
# Define numeric columns for imputation
numeric_columns <- c("Pregnancies", "Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI", "DiabetesPedigreeFunction", "Age")
# Impute missing values in training data using median of respective column
training_data[, numeric_columns] <- lapply(training_data[, numeric_columns], function(x) {
if(any(is.na(x))) x[is.na(x)] <- median(x, na.rm = TRUE)
x
})
# Impute missing values in validation data using median of corresponding column in training data
validation_data[, numeric_columns] <- lapply(names(validation_data[, numeric_columns]), function(column_name) {
columns <- validation_data[[column_name]]
if(any(is.na(columns))) {
columns[is.na(columns)] <- median(training_data[[column_name]], na.rm = TRUE)
}
return(columns)
})
# Define the new data point
new_data_point <- data.frame(Pregnancies = 4, Glucose = 178, BloodPressure = 82, SkinThickness = 32, Insulin = NA, BMI = 37, DiabetesPedigreeFunction = 0.602, Age = 54)
# Impute missing Insulin value with median from the training data
new_data_point$Insulin <- median(training_data$Insulin, na.rm = TRUE)
# Define a function to standardize columns based on mean and standard deviation
standardize <- function(column, mean_value, sd_value) {
(column - mean_value) / sd_value
}
# Calculate mean and standard deviation for each column in the training data
training_means <- sapply(training_data[selected_vars], mean, na.rm = TRUE)
training_sds <- sapply(training_data[selected_vars], sd, na.rm = TRUE)
# Standardize the new data point using means and standard deviations from training data
new_data_point[selected_vars] <- lapply(selected_vars, function(var_name) {
standardize(new_data_point[[var_name]], training_means[var_name], training_sds[var_name])
})
# Use kNN algorithm to predict the outcome for the new data point
predicted_outcome <- knn(train = training_data[, -ncol(training_data), drop = FALSE],
test = new_data_point,
cl = training_data$Outcome,
k = 5)
# Add the predicted outcome to the new data point
new_data_point$Outcome <- predicted_outcome
# Print the predicted outcome
print(predicted_outcome)
# Initialize an empty vector to store accuracy values
accuracy_values <- numeric(9)
# Loop over k values from 2 to 10
for (k_value in 2:10) {
set.seed(123) # Set seed for reproducibility
# Apply kNN algorithm to predict outcomes for validation data
predicted_results <- knn(train = training_data[, -ncol(training_data), drop = FALSE],
test = validation_data[, -ncol(validation_data), drop = FALSE],
cl = training_data$Outcome, k = k_value)
# Count correct predictions
correct_predictions_count <- sum(predicted_results == validation_data$Outcome)
# Calculate model accuracy
model_accuracy <- correct_predictions_count / nrow(validation_data)
# Store accuracy value
accuracy_values[k_value - 1] <- model_accuracy
}
# Create a data frame to store k values and corresponding accuracies
accuracy_results <- data.frame(k = 2:10, Accuracy = accuracy_values)
# Create a plot to visualize model accuracy vs. k values
ggplot(accuracy_results, aes(x = k, y = Accuracy)) +
geom_line() +
geom_point() +
scale_x_continuous(breaks = 2:10) +
labs(title = "kNN Model Accuracy vs. k", x = "Number of Neighbors (k)", y = "Accuracy") +
theme_minimal()
# Loading the dataset
abalone_data <- read.csv("/Users/nithyasarabudla/Downloads/abalone.csv")
# Extracting the target variable "Rings" and storing it in a separate vector called target_data
target_data <- abalone_data$NumRings
# Creating a new dataset train_data containing all features except "Rings"
train_data <- abalone_data[, -which(names(abalone_data) == "NumRings")]
# Encoding categorical variables
encode_data <- model.matrix(~ Sex - 1, data = train_data)
# Adding the encoded columns to the training dataset
train_data <- cbind(train_data, encode_data)
# Removing the original "Sex" column from the dataset
train_data <- train_data[, -which(names(train_data) == "Sex")]
# Removing the redundant column created by encoding (SexI)
train_data <- train_data[, -which(names(train_data) == "SexI")]
# Displaying the first few rows of the modified training dataset
head(train_data)
# Defining a function for min-max normalization
normalize <- function(x){
((x - min(x)) / (max(x) - min(x)) )
}
# Identifying numerical columns in the dataset
numerical_columns <- sapply(train_data, is.numeric)
# Applying min-max normalization to numerical columns
train_data[, numerical_columns] <- lapply(train_data[, numerical_columns], normalize)
# Displaying summary statistics of the normalized dataset
summary(train_data)
knn_regression <- function(new_data, target_data, train_data, k) {
# Calculate distances between new_data and train_data
distances <- as.matrix(dist(rbind(new_data, train_data), method = "euclidean"))[1, ]
# Find indices of k nearest neighbors
k_nearest_indices <- order(distances)[1:k]
# Extract Rings values of k nearest neighbors
k_nearest_rings <- target_data[k_nearest_indices]
# Define weights for weighted average
weights <- c(3, rep(2, k - 2), rep(1, length(k_nearest_indices) - k + 1))
# Calculate predicted Rings value using weighted average
predicted_value <- sum(k_nearest_rings * weights) / sum(weights)
# Return the predicted Rings value
return(predicted_value)
}
# Generating new_data as a data frame structured similarly to train_data
# replacing 1 in place of the "Female" column (since "Male" and "Infant" were removed during one-hot encoding)
new_data <- c(0.465,	0.356,	0.093,	0.234,	0.103,	0.14,	0.524, 0, 1)
# Predicting the Rings value for the new_data using knn_regression function with k = 3
predicted_rings <- knn_regression(new_data, target_data, train_data, k = 3)
print(predicted_rings)
# Define new_data with attributes for the new abalone
new_data <- c(0.44, 0.391, 0.254, 0.5853, 0.2132, 0.0878, 0.21, 0, 1)
# Predict the Rings value for the new abalone using knn_regression function with k = 3
predicted_rings <- knn_regression(new_data, target_data, train_data, k = 3)
# Print the predicted Rings value
print(predicted_rings)
# Start timing
start_time <- Sys.time()
# Specify the proportion of data to be used for testing
test_split <- 0.20
# Create a random split of the data into training and testing sets
train_index <- createDataPartition(target_data, p = 1 - test_split, list = FALSE)
train_data_split <- train_data[train_index, ]
test_data_split <- train_data[-train_index, ]
target_data_split <- target_data[train_index]
# Normalize numerical columns in the test dataset
test_data_split[, numerical_columns] <- lapply(test_data_split[, numerical_columns], normalize)
# Train the kNN regression model using the training data
knn_model <- knn_regression(test_data_split, target_data_split, train_data_split, k = 3)
# Calculate Mean Squared Error (MSE) to evaluate the model
mse <- mean((knn_model - target_data_split)^2)
# Calculate Root Mean Squared Error (RMSE)
rmse <- sqrt(mse)
# Print the RMSE value as a measure of the model's accuracy
print(paste("Root Mean Squared Error (RMSE):", rmse))
# End timing
end_time <- Sys.time()
# Calculate the elapsed time
elapsed_time <- end_time - start_time
print(paste("Elapsed Time:", elapsed_time))
# Load the property sales data
property_sales <- read.csv("/Users/nithyasarabudla/Downloads/property-sales.csv")
### Data Preparation and Summary Statistics
# Convert the datesold column to Date format and extract the year
property_sales$datesold <- as.Date(property_sales$datesold, "%m/%d/%y")
property_sales$year <- as.numeric(format(property_sales$datesold, "%Y"))
# Compute total number of transactions, time frame start and end
total_transactions <- nrow(property_sales)
time_frame_start <- min(property_sales$year)
time_frame_end <- max(property_sales$year)
# Calculate mean and standard deviation of property prices
# Calculate mean and standard deviation of property prices and round them
mean_price <- round(mean(property_sales$price), digits = 2)
std_dev_price <- round(sd(property_sales$price), digits = 2)
# Compute average sales price per year
average_price_per_year <- property_sales %>%
group_by(year) %>%
summarise(average_price = mean(price))
### Visualizing Average Sales Price Per Year
# Create a line plot for average sales price per year
year_average_price <- ggplot(data = average_price_per_year, aes(x = year, y = average_price)) +
geom_line() +
geom_point() +
theme_minimal() +
labs(title = "Average Sales Price Per Year",
x = "Year",
y = "Average Sales Price ($)")
### Forecasting Next Year's Sales Price
# Define weights for the weighted moving average model and compute forecast for next year
weights <- c(4, 3, 1)
last_three_years <- tail(average_price_per_year$average_price, 3)
forecast_next_year <- sum(last_three_years * weights) / sum(weights)
knitr::kable(average_price_per_year, format = "markdown")
cat("\n\n")
print(year_average_price)
mean_price
### Data Preparation and Summary Statistics
# Convert the datesold column to Date format and extract the year
property_sales$datesold <- as.Date(property_sales$datesold, "%m/%d/%y")
property_sales$year <- as.numeric(format(property_sales$datesold, "%Y"))
# Compute total number of transactions, time frame start and end
total_transactions <- nrow(property_sales)
time_frame_start <- min(property_sales$year)
time_frame_end <- max(property_sales$year)
# Calculate mean and standard deviation of property prices and round them
mean_price <- format(round(mean(property_sales$price), digits = 2), scientific = FALSE)
std_dev_price <- format(round(sd(property_sales$price), digits = 2), scientific = FALSE)
mean_price
# Define weights for the weighted moving average model and compute forecast for next year
weights <- c(4, 3, 1)
last_three_years <- tail(average_price_per_year$average_price, 3)
forecast_next_year <- sum(last_three_years * weights) / sum(weights)
# Round the forecasted next year's average sales price
forecast_next_year <- format(round(forecast_next_year, digits = 2), scientific = FALSE)
source("~/DA5030/practicum 1/practicum1_Nithya_Sarabudla.Rmd")
### Visualizing Average Sales Price Per Year
# Create a line plot for average sales price per year
year_average_price <- ggplot(data = average_price_per_year, aes(x = year, y = average_price)) +
geom_line() +
geom_point() +
theme_minimal() +
labs(title = NULL, # Remove the title
x = "Year",
y = "Average Sales Price ($)") +
theme(plot.title = element_blank()) # Remove the title completely
# Load the necessary library
library(dplyr)
library(caret)
library(ggplot2)
library(tidyr)
library(class)
# Read the dataset
diabetes <- read.csv("/Users/nithyasarabudla/Downloads/diabetes.csv")
# Display structure and summary statistics of the dataset
str(diabetes)
head(diabetes)
summary(diabetes)
glimpse(diabetes)
# Define the target columns to replace 0 values with NA
target_columns <- c("Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI")
diabetes[target_columns] <- lapply(diabetes[target_columns], function(x) replace(x, x == 0, NA))
# Remove rows with NA values in the Glucose column
diabetes <- diabetes[!is.na(diabetes$Glucose), ]
# Calculate mean and standard deviation of Glucose column
glucose_mean <- mean(diabetes$Glucose)
glucose_sd <- sd(diabetes$Glucose)
# Create a histogram of Glucose column with overlaying normal curve
ggplot(diabetes, aes(x = Glucose)) +
geom_histogram(aes(y = after_stat(density)), binwidth = 10, fill = "darkgreen", color = "black") +
stat_function(fun = dnorm, args = list(mean = glucose_mean, sd = glucose_sd),
color = "orange", linewidth=1) +
labs(title = "Histogram of Glucose Levels with Normal Curve Overlay",
x = "Glucose",
y = "Density") +
theme_minimal()
# Perform the Shapiro-Wilk test for normality on the Glucose column
shapiro_test_result <- shapiro.test(diabetes$Glucose)
# Print the result of the Shapiro-Wilk test
print(shapiro_test_result)
# Define the variables
selected_vars <- c("Pregnancies", "Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI", "DiabetesPedigreeFunction", "Age")
# Apply a function to each variable in the 'diabetes' dataset
outliers <- lapply(diabetes[selected_vars], function(var) {
# Calculate mean and standard deviation of the variable, ignoring NA values
var_mean <- mean(var, na.rm=TRUE)
var_sd <- sd(var, na.rm=TRUE)
# Calculate z-scores for each observation
z_scores <- abs((var - var_mean) / var_sd)
# Identify outliers based on z-score threshold (2.5)
outliers_index <- which(z_scores > 2.5)
# Create a data frame with indices and corresponding z-scores for outliers
return(data.frame(Indices = outliers_index, Z_Scores = z_scores[outliers_index]))
})
# Define the function for z-score standardization
standardize <- function(column) {
# Subtract the mean and divide by the standard deviation
(column - mean(column, na.rm = TRUE)) / sd(column, na.rm = TRUE)
}
# Create a copy of the original dataset
standardized_data <- diabetes
# Apply z-score standardization to all numeric columns using the defined function
standardized_data[selected_vars] <- lapply(diabetes[selected_vars], standardize)
# Extract indices of all outliers across all variables
all_outlier_indices <- unique(unlist(lapply(outliers, function(df) df$Indices)))
# Remove rows containing outliers
cleaned_data <- diabetes[-all_outlier_indices, ]
# Apply z-score standardization again to cleaned dataset
cleaned_data[selected_vars] <- lapply(cleaned_data[selected_vars], standardize)
# Print summary statistics of the original dataset
summary(diabetes)
# Set seed for reproducibility
set.seed(123)
# Create stratified sampling indices
split <- createDataPartition(cleaned_data$Outcome, p = 0.8, list = FALSE)
# Split data into training and validation sets based on the sampling indices
training_data <- cleaned_data[split, ]
validation_data <- cleaned_data[-split, ]
# Define numeric columns for imputation
numeric_columns <- c("Pregnancies", "Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI", "DiabetesPedigreeFunction", "Age")
# Impute missing values in training data using median of respective column
training_data[, numeric_columns] <- lapply(training_data[, numeric_columns], function(x) {
if(any(is.na(x))) x[is.na(x)] <- median(x, na.rm = TRUE)
x
})
# Impute missing values in validation data using median of corresponding column in training data
validation_data[, numeric_columns] <- lapply(names(validation_data[, numeric_columns]), function(column_name) {
columns <- validation_data[[column_name]]
if(any(is.na(columns))) {
columns[is.na(columns)] <- median(training_data[[column_name]], na.rm = TRUE)
}
return(columns)
})
# Define the new data point
new_data_point <- data.frame(Pregnancies = 4, Glucose = 178, BloodPressure = 82, SkinThickness = 32, Insulin = NA, BMI = 37, DiabetesPedigreeFunction = 0.602, Age = 54)
# Impute missing Insulin value with median from the training data
new_data_point$Insulin <- median(training_data$Insulin, na.rm = TRUE)
# Define a function to standardize columns based on mean and standard deviation
standardize <- function(column, mean_value, sd_value) {
(column - mean_value) / sd_value
}
# Calculate mean and standard deviation for each column in the training data
training_means <- sapply(training_data[selected_vars], mean, na.rm = TRUE)
training_sds <- sapply(training_data[selected_vars], sd, na.rm = TRUE)
# Standardize the new data point using means and standard deviations from training data
new_data_point[selected_vars] <- lapply(selected_vars, function(var_name) {
standardize(new_data_point[[var_name]], training_means[var_name], training_sds[var_name])
})
# Use kNN algorithm to predict the outcome for the new data point
predicted_outcome <- knn(train = training_data[, -ncol(training_data), drop = FALSE],
test = new_data_point,
cl = training_data$Outcome,
k = 5)
# Add the predicted outcome to the new data point
new_data_point$Outcome <- predicted_outcome
# Print the predicted outcome
print(predicted_outcome)
# Initialize an empty vector to store accuracy values
accuracy_values <- numeric(9)
# Loop over k values from 2 to 10
for (k_value in 2:10) {
set.seed(123) # Set seed for reproducibility
# Apply kNN algorithm to predict outcomes for validation data
predicted_results <- knn(train = training_data[, -ncol(training_data), drop = FALSE],
test = validation_data[, -ncol(validation_data), drop = FALSE],
cl = training_data$Outcome, k = k_value)
# Count correct predictions
correct_predictions_count <- sum(predicted_results == validation_data$Outcome)
# Calculate model accuracy
model_accuracy <- correct_predictions_count / nrow(validation_data)
# Store accuracy value
accuracy_values[k_value - 1] <- model_accuracy
}
# Create a data frame to store k values and corresponding accuracies
accuracy_results <- data.frame(k = 2:10, Accuracy = accuracy_values)
# Create a plot to visualize model accuracy vs. k values
ggplot(accuracy_results, aes(x = k, y = Accuracy)) +
geom_line() +
geom_point() +
scale_x_continuous(breaks = 2:10) +
labs(title = "kNN Model Accuracy vs. k", x = "Number of Neighbors (k)", y = "Accuracy") +
theme_minimal()
# Loading the dataset
abalone_data <- read.csv("/Users/nithyasarabudla/Downloads/abalone.csv")
# Extracting the target variable "Rings" and storing it in a separate vector called target_data
target_data <- abalone_data$NumRings
# Creating a new dataset train_data containing all features except "Rings"
train_data <- abalone_data[, -which(names(abalone_data) == "NumRings")]
# Encoding categorical variables
encode_data <- model.matrix(~ Sex - 1, data = train_data)
# Adding the encoded columns to the training dataset
train_data <- cbind(train_data, encode_data)
# Removing the original "Sex" column from the dataset
train_data <- train_data[, -which(names(train_data) == "Sex")]
# Removing the redundant column created by encoding (SexI)
train_data <- train_data[, -which(names(train_data) == "SexI")]
# Displaying the first few rows of the modified training dataset
head(train_data)
# Defining a function for min-max normalization
normalize <- function(x){
((x - min(x)) / (max(x) - min(x)) )
}
# Identifying numerical columns in the dataset
numerical_columns <- sapply(train_data, is.numeric)
# Applying min-max normalization to numerical columns
train_data[, numerical_columns] <- lapply(train_data[, numerical_columns], normalize)
# Displaying summary statistics of the normalized dataset
summary(train_data)
knn_regression <- function(new_data, target_data, train_data, k) {
# Calculate distances between new_data and train_data
distances <- as.matrix(dist(rbind(new_data, train_data), method = "euclidean"))[1, ]
# Find indices of k nearest neighbors
k_nearest_indices <- order(distances)[1:k]
# Extract Rings values of k nearest neighbors
k_nearest_rings <- target_data[k_nearest_indices]
# Define weights for weighted average
weights <- c(3, rep(2, k - 2), rep(1, length(k_nearest_indices) - k + 1))
# Calculate predicted Rings value using weighted average
predicted_value <- sum(k_nearest_rings * weights) / sum(weights)
# Return the predicted Rings value
return(predicted_value)
}
# Generating new_data as a data frame structured similarly to train_data
# replacing 1 in place of the "Female" column (since "Male" and "Infant" were removed during one-hot encoding)
new_data <- c(0.465,	0.356,	0.093,	0.234,	0.103,	0.14,	0.524, 0, 1)
# Predicting the Rings value for the new_data using knn_regression function with k = 3
predicted_rings <- knn_regression(new_data, target_data, train_data, k = 3)
print(predicted_rings)
# Define new_data with attributes for the new abalone
new_data <- c(0.44, 0.391, 0.254, 0.5853, 0.2132, 0.0878, 0.21, 0, 1)
# Predict the Rings value for the new abalone using knn_regression function with k = 3
predicted_rings <- knn_regression(new_data, target_data, train_data, k = 3)
# Print the predicted Rings value
print(predicted_rings)
# Start timing
start_time <- Sys.time()
# Specify the proportion of data to be used for testing
test_split <- 0.20
# Create a random split of the data into training and testing sets
train_index <- createDataPartition(target_data, p = 1 - test_split, list = FALSE)
train_data_split <- train_data[train_index, ]
test_data_split <- train_data[-train_index, ]
target_data_split <- target_data[train_index]
# Normalize numerical columns in the test dataset
test_data_split[, numerical_columns] <- lapply(test_data_split[, numerical_columns], normalize)
# Train the kNN regression model using the training data
knn_model <- knn_regression(test_data_split, target_data_split, train_data_split, k = 3)
# Calculate Mean Squared Error (MSE) to evaluate the model
mse <- mean((knn_model - target_data_split)^2)
# Calculate Root Mean Squared Error (RMSE)
rmse <- sqrt(mse)
# Print the RMSE value as a measure of the model's accuracy
print(paste("Root Mean Squared Error (RMSE):", rmse))
# End timing
end_time <- Sys.time()
# Calculate the elapsed time
elapsed_time <- end_time - start_time
print(paste("Elapsed Time:", elapsed_time))
# Load the property sales data
property_sales <- read.csv("/Users/nithyasarabudla/Downloads/property-sales.csv")
### Data Preparation and Summary Statistics
# Convert the datesold column to Date format and extract the year
property_sales$datesold <- as.Date(property_sales$datesold, "%m/%d/%y")
property_sales$year <- as.numeric(format(property_sales$datesold, "%Y"))
# Compute total number of transactions, time frame start and end
total_transactions <- nrow(property_sales)
time_frame_start <- min(property_sales$year)
time_frame_end <- max(property_sales$year)
# Calculate mean and standard deviation of property prices and round them
mean_price <- format(round(mean(property_sales$price), digits = 2), scientific = FALSE)
std_dev_price <- format(round(sd(property_sales$price), digits = 2), scientific = FALSE)
# Compute average sales price per year
average_price_per_year <- property_sales %>%
group_by(year) %>%
summarise(average_price = mean(price))
### Visualizing Average Sales Price Per Year
# Create a line plot for average sales price per year
year_average_price <- ggplot(data = average_price_per_year, aes(x = year, y = average_price)) +
geom_line() +
geom_point() +
theme_minimal() +
labs(title = NULL, # Remove the title
x = "Year",
y = "Average Sales Price ($)") +
theme(plot.title = element_blank()) # Remove the title completely
# Print the line graph
print(year_average_price)
# Define weights for the weighted moving average model and compute forecast for next year
weights <- c(4, 3, 1)
last_three_years <- tail(average_price_per_year$average_price, 3)
forecast_next_year <- sum(last_three_years * weights) / sum(weights)
# Round the forecasted next year's average sales price
forecast_next_year <- format(round(forecast_next_year, digits = 2), scientific = FALSE)
knitr::kable(average_price_per_year, format = "markdown")
cat("\n\n")
print(year_average_price)
### Visualizing Average Sales Price Per Year
# Create a line plot for average sales price per year
year_average_price <- ggplot(data = average_price_per_year, aes(x = year, y = average_price)) +
geom_line() +
geom_point() +
theme_minimal() +
labs(title = NULL, # Remove the title
x = "Year",
y = "Average Sales Price ($)") +
theme(plot.title = element_blank()) # Remove the title completely
# Print the line graph
print(year_average_price)
source("~/DA5030/practicum 1/DA5030.P1.Sarabudla.Rmd")
